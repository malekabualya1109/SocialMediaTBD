{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4e51640-b956-4937-af66-3fd0dccf3ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a3ee2a3f-6243-4c08-9e95-63438e24d8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5392 entries, 0 to 5391\n",
      "Columns: 110 entries, id to in_youtubers\n",
      "dtypes: float64(2), int64(104), object(4)\n",
      "memory usage: 4.5+ MB\n"
     ]
    }
   ],
   "source": [
    "#loading the data set\n",
    "try:\n",
    "    df = pd.read_csv(\"talent.csv\")\n",
    "except Exception as e:\n",
    "    print(\"Error reading CSV file:\", e)\n",
    "    sys.exit(1)\n",
    "\n",
    "df.info()\n",
    "sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f0d68629-11f6-4852-b67e-ca5db829e39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Duplicate names found:\n",
      "               name\n",
      "989   Renee Olstead\n",
      "2326   Brooke Berry\n",
      "2600           Kyle\n",
      "2793           Kyle\n",
      "4116             JP\n",
      "4175             JP\n",
      "4182  Amber Diamond\n",
      "4383  Renee Olstead\n",
      "4443  Amber Diamond\n",
      "4969   Brooke Berry\n",
      "Dropped duplicates, keeping first occurrence.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "binary_cols = [col for col in df.columns if col.startswith(\"in_\")]\n",
    "df[binary_cols] = df[binary_cols].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "df['name'] = df['name'].astype(str).str.strip()\n",
    "\n",
    "# handling duplicates\n",
    "duplicates = df[df['name'].duplicated(keep=False)]\n",
    "if not duplicates.empty:\n",
    "    print(\"Warning: Duplicate names found:\")\n",
    "    print(duplicates[['name']])\n",
    "    df = df.drop_duplicates(subset='name', keep='first')\n",
    "    print(\"Dropped duplicates, keeping first occurrence.\")\n",
    "\n",
    "# checking non-binary values\n",
    "non_binary = [col for col in binary_cols if not df[col].isin([0, 1, np.nan]).all()]\n",
    "if non_binary:\n",
    "    print(\"Warning: Non-binary values found in columns:\", non_binary)\n",
    "\n",
    "# Check sparsity and category distribution\n",
    "sparsity = (df[binary_cols] == 0).mean().mean()\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d795b363-df6f-4226-96e0-c904bdff73fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new DataFrame X that includes the 'stars' column (a quality score)\n",
    "# along with a set of binary columns (e.g., features that are either True/False)\n",
    "X = df[['stars'] + binary_cols].copy()\n",
    "\n",
    "# normalizing the 'stars' column:\n",
    "# subtracting the mean and dividing by the standard deviation.\n",
    "# We add a small number (1e-6) to the standard deviation so we never divide by zero.\n",
    "X['stars'] = (X['stars'] - X['stars'].mean()) / (X['stars'].std() + 1e-6)\n",
    "\n",
    "# Calculating moderate weights for the binary columns:\n",
    "# 1. For each binary column, sum its values (to see how common each feature is).\n",
    "# 2. Take the reciprocal (1/count) so that features appearing rarely get higher weight.\n",
    "# 3. Use np.log1p to apply a logarithm that smooths out the weights.\n",
    "# 4. Clip the weights between 0 and 10 to avoid extreme values.\n",
    "weights = np.log1p(1 / (df[binary_cols].sum() + 1e-6)).clip(0, 10)\n",
    "\n",
    "# Add a default weight of 1 for the 'stars' column.\n",
    "# We combine this with the previously calculated weights for the binary columns.\n",
    "weights = pd.concat([pd.Series(1.0, index=['stars']), weights])\n",
    "\n",
    "# Multiply each column in our feature matrix X by its corresponding weight.\n",
    "# This creates a weighted feature matrix where more important features have a bigger effect.\n",
    "X_weighted = X.mul(weights)\n",
    "\n",
    "# Compute the cosine similarity between every pair of rows in the weighted feature matrix.\n",
    "# The result is a similarity matrix indicating how similar each pair is.\n",
    "similarity_matrix = cosine_similarity(X_weighted)\n",
    "\n",
    "# Make sure all similarity values are in the range [0, 1].\n",
    "# This might be needed in case some small numerical issues cause values to slightly exceed these boundaries.\n",
    "similarity_matrix = np.clip(similarity_matrix, 0, 1)\n",
    "\n",
    "# Convert the similarity matrix (a NumPy array) back into a DataFrame.\n",
    "# Use the names from df['name'] as both the row and column labels.\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=df['name'], columns=df['name'])\n",
    "\n",
    "# Check if any names from the original data are missing in the similarity DataFrame.\n",
    "missing = set(df['name']) - set(similarity_df.index)\n",
    "if missing:\n",
    "    # If names are missing, print a warning message.\n",
    "    print(f\"Warning: Missing names in similarity_df: {missing}\")\n",
    "\n",
    "# Ensure that all printed output is sent to the command prompt immediately.\n",
    "sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5b19e82c-9e32-4b95-85f3-7ff3ea28604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def recommend_similar_items(item_name, sim_df, df, binary_cols, top_n=5):\n",
    "    # Check if the specified item exists in the similarity DataFrame.\n",
    "    # If not, raise an error and show some example names.\n",
    "    if item_name not in sim_df.index:\n",
    "        raise KeyError(f\"'{item_name}' not in similarity DataFrame index. Try: {list(sim_df.index[:5])}\")\n",
    "    \n",
    "    # Retrieve the similarity scores for the given item.\n",
    "    # This row contains the similarity between the given item and every other item.\n",
    "    sim_scores = sim_df.loc[item_name]\n",
    "    \n",
    "    # In some cases there might be multiple entries for the same item\n",
    "    # (for example, duplicate rows). If so, warn the user and use the first entry.\n",
    "    if isinstance(sim_scores, pd.DataFrame):\n",
    "        print(f\"Warning: Multiple entries for '{item_name}'. Using first row.\")\n",
    "        sim_scores = sim_scores.iloc[0]\n",
    "    \n",
    "    # Remove the similarity score for the item with itself.\n",
    "    # This prevents the function from recommending the same item as its own similar item.\n",
    "    sim_scores = sim_scores.drop(item_name, errors='ignore')\n",
    "    \n",
    "    # Sort the remaining scores in descending order to find the items most similar to the given item.\n",
    "    # Then, select the top_n items with the highest similarity.\n",
    "    top_items = sim_scores.sort_values(ascending=False).head(top_n)\n",
    "    \n",
    "    # If the highest similarity score is extremely low (close to zero),\n",
    "    # assume that no similar items were found.\n",
    "    # In that case, fall back to recommending the most popular items based on the 'stars' rating.\n",
    "    if top_items.max() < 1e-6:\n",
    "        top_items = df.nlargest(top_n, 'stars')['name']\n",
    "        # Create a series of zeros (indicating similarity scores) for the fallback items.\n",
    "        top_items = pd.Series([0.0] * top_n, index=top_items)\n",
    "    \n",
    "    # Return the final series of recommended items and their similarity scores.\n",
    "    return top_items\n",
    "\n",
    "\n",
    "def recommend_for_user(user_interests, X, item_names, df, top_n=5):\n",
    "    # Check if each user interest corresponds to a valid column in the feature matrix X.\n",
    "    # Build a list of interests that are not found in X.\n",
    "    invalid_interests = [i for i in user_interests if i not in X.columns]\n",
    "    \n",
    "    # If there are any invalid interests, warn the user and remove them.\n",
    "    if invalid_interests:\n",
    "        print(f\"Warning: Invalid interests {invalid_interests}. Ignoring.\")\n",
    "        user_interests = [i for i in user_interests if i in X.columns]\n",
    "    \n",
    "    # If after filtering there are no valid interests left,\n",
    "    # return the most popular items based on their 'stars' ratings.\n",
    "    if not user_interests:\n",
    "        print(\"No valid interests. Returning popular items.\")\n",
    "        return df.nlargest(top_n, 'stars')['name'].to_series()\n",
    "    \n",
    "    # Create a binary vector representing the user's interests.\n",
    "    # For each column in X, place a 1 if the user is interested in that feature, otherwise 0.\n",
    "    user_vector = np.array([1 if col in user_interests else 0 for col in X.columns])\n",
    "    \n",
    "    # Calculate cosine similarity between each item in X and the user's interest vector.\n",
    "    # This produces a score for each item, indicating how closely it matches the user's interests.\n",
    "    sim_scores = cosine_similarity(X, [user_vector]).flatten()\n",
    "    \n",
    "    # Convert the similarity scores into a pandas Series for easier handling, \n",
    "    # using item_names as the index so each score corresponds to an item.\n",
    "    sim_series = pd.Series(sim_scores, index=item_names)\n",
    "    \n",
    "    # If any values in the similarity scores are NaN, replace them with zero.\n",
    "    sim_series = sim_series.fillna(0)\n",
    "    \n",
    "    # Count how many of the user's interests apply to each item by summing the valid features.\n",
    "    # This provides an extra measure of how well each item matches the user's preferences.\n",
    "    interest_counts = df[user_interests].sum(axis=1)\n",
    "    \n",
    "    # Enhance the similarity scores by slightly increasing the value based on the number of matching interests.\n",
    "    # The more interests an item matches, the higher its score becomes (with a 10% boost per interest match).\n",
    "    sim_series = sim_series * (1 + 0.1 * interest_counts)\n",
    "    \n",
    "    # Sort the boosted scores in descending order and select the top_n items.\n",
    "    top_items = sim_series.sort_values(ascending=False).head(top_n)\n",
    "    \n",
    "    # If the maximum similarity after boosting is still extremely low,\n",
    "    # it's likely that none of the items match the user's interests well.\n",
    "    # In that case, fall back to returning the most popular items.\n",
    "    if top_items.max() < 1e-6:\n",
    "        print(\"No matching items. Returning popular items.\")\n",
    "        top_items = df.nlargest(top_n, 'stars')['name'].to_series()\n",
    "    \n",
    "    # Return the final list of recommended items with their scores.\n",
    "    return top_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7271809a-4b73-4058-beac-f10ca294bb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def precision_at_k(item_name, sim_df, true_df, binary_cols, k=5):\n",
    "    \"\"\"\n",
    "    Compute the precision at k for a given item.\n",
    "    Precision at k measures the fraction of recommended items (out of k)\n",
    "    that share at least one common category with the target item.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Generate the top k recommended items for the given item_name.\n",
    "        recs = recommend_similar_items(item_name, sim_df, true_df, binary_cols, k)\n",
    "        \n",
    "        # Find the set of true categories (binary features with value 1)\n",
    "        # for the target item. This uses the true_df DataFrame.\n",
    "        target_item_rows = true_df[true_df['name'] == item_name]\n",
    "        # Filter binary columns where the value is 1 and retrieve the column names\n",
    "        true_cats = set(\n",
    "            target_item_rows[binary_cols].columns[\n",
    "                target_item_rows[binary_cols].iloc[0] == 1\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # If the target item has no true categories marked, precision is 0.\n",
    "        if not true_cats:\n",
    "            return 0\n",
    "        \n",
    "        # Count how many of the recommended items share at least one category\n",
    "        # with the target item's true categories.\n",
    "        relevant_count = 0\n",
    "        for rec_name in recs.index:\n",
    "            rec_item_rows = true_df[true_df['name'] == rec_name]\n",
    "            # Retrieve the set of active categories for the recommended item.\n",
    "            rec_cats = set(\n",
    "                rec_item_rows[binary_cols].columns[\n",
    "                    rec_item_rows[binary_cols].iloc[0] == 1\n",
    "                ]\n",
    "            )\n",
    "            # Check if there is any overlap between the target and recommended categories.\n",
    "            if true_cats.intersection(rec_cats):\n",
    "                relevant_count += 1\n",
    "        \n",
    "        # Calculate precision as the fraction of recommended items that are relevant.\n",
    "        return relevant_count / k\n",
    "    \n",
    "    except KeyError:\n",
    "        # If the item is not found in the similarity DataFrame, return 0.\n",
    "        return 0\n",
    "        \n",
    "def compute_coverage(sim_df, df, binary_cols, top_n=5):\n",
    "    \"\"\"\n",
    "    Compute the coverage of the recommendation system.\n",
    "    Coverage is defined as the proportion of items in the dataset\n",
    "    that appear in any recommendation list.\n",
    "    \"\"\"\n",
    "    all_recs = set()\n",
    "    \n",
    "    # Iterate over each item name in the dataset\n",
    "    for name in df['name']:\n",
    "        try:\n",
    "            # Get the top_n recommendations for the current item.\n",
    "            recs = recommend_similar_items(name, sim_df, df, binary_cols, top_n)\n",
    "            # Add the recommended items to the overall set of recommendations.\n",
    "            all_recs.update(recs.index)\n",
    "        except KeyError:\n",
    "            # If the item is not found in the similarity DataFrame, skip it.\n",
    "            continue\n",
    "    \n",
    "    # Calculate the coverage ratio.\n",
    "    # If there are items in the dataset, divide the number of unique recommendations\n",
    "    # by the total number of items. Return 0 if the dataset is empty.\n",
    "    return len(all_recs) / len(df) if len(df) > 0 else 0\n",
    "\n",
    "def intra_list_similarity(rec_names, sim_df):\n",
    "    \"\"\"\n",
    "    Calculate the average cosine similarity among all unique pairs of items\n",
    "    in a recommendation list. This measures how similar the recommended items\n",
    "    are to each other.\n",
    "    \"\"\"\n",
    "    # If there are fewer than 2 recommendations, no pair exists, so return 0.\n",
    "    if len(rec_names) < 2:\n",
    "        return 0\n",
    "    \n",
    "    # Compute similarity scores for each unique pair in the recommendation list.\n",
    "    sim_scores = [\n",
    "        sim_df.loc[n1, n2] \n",
    "        for i, n1 in enumerate(rec_names) \n",
    "        for n2 in rec_names[i+1:]\n",
    "    ]\n",
    "    \n",
    "    # Return the mean of the similarity scores if any exist; otherwise, return 0.\n",
    "    return np.mean(sim_scores) if sim_scores else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3af44719-1898-455a-b569-a5c532bc3412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommendations for 'Perez Hilton':\n",
      "name\n",
      "Kristen Ledlow     0.999996\n",
      "Darren Rovell      0.999994\n",
      "Matt Hasselbeck    0.999984\n",
      "Bob Menery         0.999978\n",
      "Brian Balthazar    0.999957\n",
      "Name: Perez Hilton, dtype: float64\n",
      "Categories of recommended items:\n",
      "in_commentators     5\n",
      "in_featured         4\n",
      "in_athletes         1\n",
      "in_comedians        1\n",
      "in_football         1\n",
      "in_reality_tv       1\n",
      "in_90_day_fiance    0\n",
      "in_artists          0\n",
      "in_animals          0\n",
      "in_american_idol    0\n",
      "dtype: int64\n",
      "Intra-list diversity: 0.000\n",
      "\n",
      "Recommendations for interests ['in_90_day_fiance', 'in_artists']:\n",
      "0   NaN\n",
      "1   NaN\n",
      "2   NaN\n",
      "3   NaN\n",
      "4   NaN\n",
      "dtype: float64\n",
      "Interest flags for recommended items:\n",
      "Empty DataFrame\n",
      "Columns: [in_90_day_fiance, in_artists]\n",
      "Index: []\n",
      "\n",
      "Items with both ['in_90_day_fiance', 'in_artists']: 0\n",
      "\n",
      "Average Precision@5: 0.999\n",
      "Coverage: 0.681\n",
      "Average stars of recommendations: nan (dataset avg: 3.69)\n",
      "Average reactions of recommendations: nan (dataset avg: 10.07)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Choose an item to use as a query for recommendations.\n",
    "# Here, we select the first valid name from the 'name' column of our DataFrame.\n",
    "item_of_interest = df['name'].iloc[0]  # First valid name\n",
    "\n",
    "try:\n",
    "    # Get top 5 similar items for the chosen item using our recommendation function.\n",
    "    recs = recommend_similar_items(item_of_interest, similarity_df, df, binary_cols, top_n=5)\n",
    "    \n",
    "    # Print a header and the list of recommended items for the selected item.\n",
    "    print(f\"\\nRecommendations for '{item_of_interest}':\")\n",
    "    print(recs)\n",
    "    \n",
    "    # For the recommended items, sum up the binary category flags to understand\n",
    "    # which categories/features are most common among them.\n",
    "    print(\"Categories of recommended items:\")\n",
    "    print(df[df['name'].isin(recs.index)][binary_cols].sum().sort_values(ascending=False).head(10))\n",
    "    \n",
    "    # Calculate intra-list diversity as 1 minus the average cosine similarity among recommendations.\n",
    "    # This metric helps assess how varied (or diverse) the recommendations are.\n",
    "    diversity = 1 - intra_list_similarity(recs.index, similarity_df)\n",
    "    print(f\"Intra-list diversity: {diversity:.3f}\")\n",
    "    \n",
    "except KeyError as e:\n",
    "    # If the item_of_interest is not found in the similarity DataFrame, catch the error and print it.\n",
    "    print(\"KeyError:\", e)\n",
    "\n",
    "# Flush the output buffer to ensure all prints are displayed immediately.\n",
    "sys.stdout.flush()\n",
    "\n",
    "\n",
    "# Define a list of user interests using the corresponding binary feature labels.\n",
    "user_interests = [\"in_90_day_fiance\", \"in_artists\"]\n",
    "\n",
    "try:\n",
    "    # Generate personalized recommendations based on the user's interests.\n",
    "    # The function returns the top 5 items that best match the given interests.\n",
    "    user_recs = recommend_for_user(user_interests, X, df['name'], df, top_n=5)\n",
    "    \n",
    "    # Display the recommendations for the given interests.\n",
    "    print(f\"\\nRecommendations for interests {user_interests}:\")\n",
    "    print(user_recs)\n",
    "    \n",
    "    # Also show the interest flags for the recommended items to verify they align\n",
    "    # with the user's interests.\n",
    "    print(\"Interest flags for recommended items:\")\n",
    "    print(df[df['name'].isin(user_recs.index)][user_interests])\n",
    "    \n",
    "except Exception as e:\n",
    "    # Catch any error during the recommendation process and print an error message.\n",
    "    print(\"Error in user recommendations:\", e)\n",
    "\n",
    "\n",
    "# Identify items in the dataset that have all of the specified user interest flags enabled.\n",
    "both_interests = df[df[user_interests].eq(1).all(axis=1)]['name']\n",
    "print(f\"\\nItems with both {user_interests}: {len(both_interests)}\")\n",
    "if not both_interests.empty:\n",
    "    # If any items match, display the first few.\n",
    "    print(both_interests.head())\n",
    "\n",
    "\n",
    "# Evaluate the recommendation system's precision using the full similarity DataFrame.\n",
    "# We compute precision@k for a 20% random sample of items to speed up the process.\n",
    "precision_scores = []\n",
    "for name in df['name'].sample(frac=0.2, random_state=42):\n",
    "    # Calculate precision@5 for each sampled item.\n",
    "    precision = precision_at_k(name, similarity_df, df, binary_cols, k=5)\n",
    "    if precision is not None:\n",
    "        precision_scores.append(precision)\n",
    "        \n",
    "# Compute the average precision over all sampled items.\n",
    "avg_precision = np.mean(precision_scores) if precision_scores else 0\n",
    "print(f\"\\nAverage Precision@5: {avg_precision:.3f}\")\n",
    "\n",
    "# Compute coverage, which measures the proportion of items that appear in any recommendation list.\n",
    "coverage = compute_coverage(similarity_df, df, binary_cols, top_n=5)\n",
    "print(f\"Coverage: {coverage:.3f}\")\n",
    "\n",
    "\n",
    "# comparing the average stars and reactions of the recommended items\n",
    "# to the overall dataset averages if those columns are available.\n",
    "if 'stars' in df.columns and 'reactions' in df.columns:\n",
    "    try:\n",
    "        # Get the names of the items recommended based on user interests.\n",
    "        rec_names = user_recs.index\n",
    "        \n",
    "        # Calculate average stars and reactions for these recommended items.\n",
    "        avg_stars = df[df['name'].isin(rec_names)]['stars'].mean()\n",
    "        avg_reactions = df[df['name'].isin(rec_names)]['reactions'].mean()\n",
    "        \n",
    "        # Also compute the average stars and reactions for the entire dataset.\n",
    "        dataset_stars = df['stars'].mean()\n",
    "        dataset_reactions = df['reactions'].mean()\n",
    "        \n",
    "        # Print the comparison to see if the recommendations skew toward higher quality or popularity.\n",
    "        print(f\"Average stars of recommendations: {avg_stars:.2f} (dataset avg: {dataset_stars:.2f})\")\n",
    "        print(f\"Average reactions of recommendations: {avg_reactions:.2f} (dataset avg: {dataset_reactions:.2f})\")\n",
    "        \n",
    "    except:\n",
    "        # If any error occurs while calculating these metrics, print an error message.\n",
    "        print(\"Error computing stars/reactions\")\n",
    "\n",
    "# Flush all pending print outputs to the console.\n",
    "sys.stdout.flush()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
